---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.14.1
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Web Scraping

```{code-cell} ipython3
import requests
from bs4 import BeautifulSoup
import pandas as pd
```


````{warning}
If it says it cannot load one of the libraries, use pip inside your notebook to install, then restart your kernel (Kernel menu, choose restart)

```
pip install beautifulsoup4
```
````

## Getting Data From Websites 


We have seen that `read_html` can get content from an actual website, not a data file that is hosted somewhere on the internet, that takes tables on a website and returns a list of DataFrames.

````{margin}
```{note}
This has a long output, so I hid it by default, but you can view it
```
````

```{code-cell} ipython3
:tags: ["hide-output"]
pd.read_html('https://rhodyprog4ds.github.io/BrownSpring23/syllabus/achievements.html')
```

This gives us a list of DataFrames that come from the website.  `pandas` gets tables by looking in the html for the site and finding the `<table>` tags. 


## Everything is Data

For the purpose of this class, it is best to think of the content on a web page like a datastructure.  

![html anatomy](https://cdn2.hubspot.net/hub/53/file-1519188549-jpg/blog-files/webpage-setup.jpg)


![HTML tree structure](http://www.w3schools.com/js/pic_htmltree.gif)

there are tags `<>` that define the structure, and these can be further classified with `classes`



## Scraping a URI website


We're going to create a DataFrame about URI CS & Statistics Faculty.

from the [people page](https://web.uri.edu/cs/people/) of the department website.

````{margin}
```{note}
the inspect link goes to instructions for different browsers
```
````
We can [inspect](https://blog.hubspot.com/website/how-to-inspect) the page to check that it's well structured.  

```{warning}
With great power comes great responsibility.

- always check the [robots.txt](https://web.uri.edu/robots.txt)
- do not do things that the owner says not to do
- government websites are typically safe
```


We'll save the URL for easy use

Then we can use the `requests` library to make a call to the internet.  It actually gets back a [response object](https://requests.readthedocs.io/en/latest/api/#requests.Response) which has a lot of extra information.  For today we only need the `content` from the page which is an attrtibute of that object

```{code-cell} ipython3
cs_people_url = 'https://web.uri.edu/cs/people/'
cs_people_html = requests.get(cs_people_url).content

cs_people = BeautifulSoup(cs_people_html,'html.parser')
```

This is raw: 
````{margin}
```{note}
here I suppressed th output in class by looking only at the first few characters
```
````

```{code-cell} ipython3
cs_people_html[:100]
```


But we do not need to manually write search tools, that's what [`BeautifulSoup`](https://beautiful-soup-4.readthedocs.io/en/latest/) is for.
```{code-cell} ipython3
:tags: ["hide-output"]
cs_people
```

### Looking at tags 

In this object we can use any tag from the file and get back the first instance
```{code-cell} ipython3
cs_people.a
```

```{code-cell} ipython3
cs_people.div
```

```{code-cell} ipython3
cs_people.h3
```

this [cheatsheet](https://web.stanford.edu/group/csp/cs21/htmlcheatsheet.pdf) shows lots of html tags, but for this purpose you do not really need it. You'll be inspecting the page and then looking for what you want

### Searching the source


More helpful is the `find_all` method we wnat to find all `div` tags that are "peopleitem" class. We decided this by inspecting the code on the website.

```{code-cell} ipython3
:tags: ["hide-output"]
cs_people.find_all('div','peopleitem')

```
this is a long, object and we can see it looks iterable (`[` at the start)

```{code-cell} ipython3
people_items = cs_people.find_all('div','peopleitem')
len(people_items)
```


```{important}
answer to questions about searching [from the docs](https://beautiful-soup-4.readthedocs.io/en/latest/index.html?highlight=find_all#the-keyword-arguments)
```


```{code-cell} ipython3
type(people_items)
```
We can also look at only the first instance
```{code-cell} ipython3
people_items[0]
```

We notice that the name is inside a `<h3>` tag with class `p-name` and then inside an a tag after that.  We also know from looking at the overall page that there are lots of other a tags, so we do not want to search all of those.

```{code-cell} ipython3
people_items[0].find('h3','p-name').a.string
```

Then we can use a list comprehension to build alist of them. 

```{code-cell} ipython3
names = [name.a.string for name in cs_people.find_all('h3','p-name')]
names
```

```{code-cell} ipython3
people_items[0]
```

```{code-cell} ipython3
people_items[0].find('p','people-title').string
```

How to pull out the titles for each person (eg Assitatn Teaching Professor, Associate Professor)

```{code-cell} ipython3
titles = [t.string for t in cs_people.find_all("p","people-title")]
```

```{code-cell} ipython3
titles
```

on one item, the `p` tag seems to work, but that is because the tag gives only the first instance, 
```{code-cell} ipython3
people_items[0].find('p')
```

but we see if we ust this for all, it is way more informaiton than we were looking for. 
```{code-cell} ipython3
[t.string for t in cs_people.find_all("p")]
```

We can pull out two more things, the people-department indicates who is CS & who is Statistics.
```{code-cell} ipython3
disciplines = [d.string for d in cs_people.find_all("p",'people-department')]
emails = [e.string for e in cs_people.find_all("a",'u-email')]
```

```{code-cell} ipython3
css_df = pd.DataFrame({'name':names, 'title':titles,'e-mails':emails, 'discipline':disciplines})
css_df.head()
```

## Crawling and scraping

Remember we pulled the names out of links, when in the browser, we click on the links, we see that they are to a profile page. On these pages, they have the office number.  Let's add those to our dataframe. 

First, we will do it for one person, then make a loop. 

```{code-cell} ipython3
people_items[0].find('h3','p-name').a
```
We see that the information that we want is in the `href` attribute, to read that, we check the [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#attributes). This tells us there is a `.attrs` attribute of the python object we are working with. 


```{code-cell} ipython3
people_items[0].find('h3','p-name').a.attrs
```

It's a dictionary and the attribute we want is the key we want. Nowe, we do the same thing we did above, request, pull the content from the response and then use the parser. 

```{code-cell} ipython3
alvarez_url = people_items[0].find('h3','p-name').a.attrs['href']
alvarez_html = requests.get(alvarez_url).content
alvarez_info = BeautifulSoup(alvarez_html,'html.parser')
```

then we find the tag and class we need from inspecting and pull that. 
```{code-cell} ipython3
alvarez_info.find_all('li','people-location')
```

it's an interable, so we pull the item out
```{code-cell} ipython3
alvarez_info.find_all('li','people-location')[0]
```

Then we get the content. 
```{code-cell} ipython3
alvarez_info.find_all('li','people-location')[0].string
```
this time this doesn't work, so we can use the python `__dict__` to inspect the object and see where it stored what we want.  

```{code-cell} ipython3
alvarez_info.find_all('li','people-location')[0].__dict__
```

it's the second elment of content
```{code-cell} ipython3
alvarez_info.find_all('li','people-location')[0].contents[1]
```


Now tht we know how to do it, we can put it in a loop.  

```{code-cell} ipython3
offices = []
for name_link in cs_people.find_all('h3','p-name'):
    url = name_link.a.attrs['href']
    person_html = requests.get(url).content
    person_info = BeautifulSoup(person_html,'html.parser')
    try: 
        offices.append(person_info.find_all('li','people-location')[0].contents[1])
    except:
        offices.append(pd.NA)


css_df['office'] = offices
```

We got an error at first, so we added the [`try` and `except`](https://docs.python.org/3/tutorial/errors.html#handling-exceptions) to handle when there is no office location. 

```{code-cell} ipython3
css_df.head()
```

```{code-cell} ipython3
css_df.to_csv('css_faculty.csv')
```

```{code-cell} ipython3

```
## Questions after class

### what does .a do?
it gives the first instance of the `<a>` tag 

### is it worth it to try and web scrape a page that is poorly written?

If it is important information.  In these cases, you might have to do more manual parsing or even some manual fixes. 

For this class, no. 

### In theory, you could parse images and potentially their metadata with this method?

This method could be a way to download images and the text that is around them, yes.  This is how a lot of image datasets are built for machine learning. 

### Is API the website's way of specify what information it will allow for you have? 

What we did today did not use any API.  An API call would use the request library, and similar patterns to what we did, espeically the end of class. However and API call would typically respond with json, not html. 

### In the web-scraping of the offices, there were two strings, 'CBLS 377', and 'CBLS Building 487' how would we use pandas to normalize things like this?‚Äù

We could use the `replace` method that we used last week. 